# ğŸ“Š YouTube Comment Sentiment Analyzer  
*A Real-Time Sentiment Analysis & Visualization System using PySpark, NLTK, and Streamlit*

---

## ğŸ§¾ Version Information

| Component | Version | Description |
|------------|----------|-------------|
| **Python** | 3.11+ | Programming language used |
| **PySpark** | 3.5.7 | Distributed data processing engine |
| **NLTK (VADER)** | 3.9.2 | Natural language sentiment analysis |
| **TextBlob** | 0.17+ | Secondary sentiment analyzer |
| **Pandas** | 2.3.3 | Data manipulation and cleaning |
| **Streamlit** | 1.51.0 | Interactive dashboard framework |
| **Plotly** | 6.4.0 | Data visualization library |
| **Google API Client** | 2.187.0 | Used for YouTube Data API access |
| **Schedule** | 1.2.2 | Automates periodic data fetching |
| **dotenv** | 1.0+ | Environment variable management |

**Latest Project Version:** `v1.0.0`  
**Release Date:** November 2025

---

## ğŸš€ Overview

**YouTube Comment Sentiment Analyzer** is an end-to-end NLP and data engineering project that:
- Fetches live YouTube comments through the **YouTube Data API v3**
- Processes and analyzes comment text using **PySpark** and **NLTK VADER**
- Computes **sentiment scores**, **engagement levels**, and **text-based features**
- Visualizes the results interactively using **Streamlit dashboards**

This project combines **real-time data ingestion**, **AI-powered NLP sentiment analysis**, and **visual storytelling** for deep insights into YouTube engagement.

---

## ğŸ§© Key Modules

### 1. **`youtube_fetcher.py`**
- Fetches comments using the **YouTube Data API v3**
- Extracts details such as:
  - `comment_id`
  - `author`
  - `text`
  - `likes`
  - `timestamp`
- Merges fetched comments into CSV files, removing duplicates  

### 2. **`data_processor.py`**
- Processes and cleans the comment data using **PySpark**
- Adds multiple layers of feature engineering:
  - Sentiment classification (VADER)
  - Engagement metrics (likes + replies)
  - Timestamp-based features (hour, weekday)
  - Text features (word count, emoji presence)
- Outputs a cleaned and enriched dataset for analytics  

### 3. **`sentiment_analyzer.py`**
- Performs text-based sentiment analysis using:
  - **VADER (from NLTK)** for social-media-optimized sentiment scoring  
  - **TextBlob** for polarity and subjectivity metrics  
- Supports batch and dataframe-based sentiment analysis  
- Returns key metrics: `positive`, `neutral`, `negative`, and `compound score`  

### 4. **`dashboard.py`**
- A **Streamlit dashboard** for visual exploration  
- Includes:
  - Sentiment Distribution (Pie/Bar)
  - Sentiment Trends Over Time
  - Engagement vs Sentiment Heatmap
  - Text Statistics (Word Count, Text Length)
  - Top Comments by Likes
- Interactive filters for date and sentiment  
- CSV export for analyzed data  

### 5. **`main.py`**
- Core orchestrator of the full pipeline:
  1. Fetch â†’ 2. Process â†’ 3. Analyze â†’ 4. Save  
- Integrates **PySpark**, **YouTubeFetcher**, and **SparkDataProcessor**  
- Handles continuous comment fetching via the `schedule` library  

---

## ğŸ§± Project Architecture
```
youtube-pyspark/
â”‚
â”œâ”€â”€ main.py # Main orchestrator for data fetching & Spark processing
â”œâ”€â”€ youtube_fetcher.py # Fetches comments using YouTube API
â”œâ”€â”€ data_processor.py # Data cleaning, feature engineering (PySpark)
â”œâ”€â”€ sentiment_analyzer.py # Sentiment scoring using NLTK/TextBlob
â”œâ”€â”€ dashboard.py # Streamlit-based interactive visualization
â”œâ”€â”€ config.py # Configuration for paths and parameters
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw_comments.csv
â”‚ â””â”€â”€ processed_sentiment.csv
â”‚
â”œâ”€â”€ models/ # Placeholder for ML models
â”œâ”€â”€ logs/ # Runtime logs
â”‚
â”œâ”€â”€ .env # Contains YouTube API key (excluded from Git)
â”œâ”€â”€ .gitignore # Prevents committing unnecessary files
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # Documentation
```

---

## âš™ï¸ Configuration

### **`config.py`**
```python
CONFIG = {
    "max_comments_per_video": 1000,
    "fetch_interval": 300,       # 5 minutes
    "sentiment_threshold": 0.5,
    "batch_size": 100,
    "languages": ["en"],
    "update_mode": "append",
}

FILE_PATHS = {
    "raw_comments": "data/raw_comments.csv",
    "processed_sentiment": "data/processed_sentiment.csv",
    "processed_data": "data/processed_data.csv",
    "model_path": "models/sentiment_model.pkl",
    "logs": "logs/",
}
```

---

## ğŸ“‹ Prerequisites

Before running this project, ensure you have:

1. **Python 3.11+** installed
2. **YouTube Data API v3 Key** - [Get it here](https://console.developers.google.com/)
3. **Java 8 or 11** (required for PySpark)
4. **Git** (for cloning the repository)

---

## ğŸ› ï¸ Installation & Setup

### Step 1: Clone the Repository
```bash
git clone https://github.com/yourusername/youtube-pyspark.git
cd youtube-pyspark
```

### Step 2: Create Virtual Environment
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 4: Download NLTK Data
```bash
python -c "import nltk; nltk.download('vader_lexicon'); nltk.download('punkt')"
```

### Step 5: Set Up Environment Variables
Create a `.env` file in the root directory:
```env
YOUTUBE_API_KEY=your_youtube_api_key_here
```

### Step 6: Create Required Directories
```bash
mkdir data logs models notebooks
```

---

## ğŸ¯ Usage

### Option 1: Single Analysis (One-Time Fetch)
```bash
python main.py "https://www.youtube.com/watch?v=VIDEO_ID"
```

### Option 2: Continuous Analysis (Periodic Updates)
```bash
python main.py "https://www.youtube.com/watch?v=VIDEO_ID" --continuous --interval 5
```
*Fetches new comments every 5 minutes*

### Option 3: Launch Interactive Dashboard
```bash
streamlit run dashboard.py
```
*Opens at `http://localhost:8501`*

---

## ğŸ“Š Dashboard Features

The Streamlit dashboard provides:

- **ğŸ“ˆ Sentiment Distribution** - Pie chart and bar chart visualization
- **â° Sentiment Trends Over Time** - Line chart showing sentiment changes
- **ğŸ”¥ Engagement Heatmap** - Correlation between sentiment and engagement
- **ğŸ“ Text Statistics** - Word count, text length, emoji analysis
- **â­ Top Comments** - Highest-liked comments with sentiment scores
- **ğŸ” Interactive Filters** - Filter by date range and sentiment type
- **ğŸ’¾ CSV Export** - Download processed data for further analysis

---

## ğŸ§ª Example Workflow

```python
# 1. Fetch comments from a YouTube video
from youtube_fetcher import YouTubeFetcher

fetcher = YouTubeFetcher()
comments_df = fetcher.save_comments_to_csv("https://www.youtube.com/watch?v=dQw4w9WgXcQ")

# 2. Process with PySpark
from pyspark.sql import SparkSession
from data_processor import SparkDataProcessor

spark = SparkSession.builder.appName("SentimentAnalysis").getOrCreate()
processor = SparkDataProcessor(spark)
df_spark = processor.load_data("data/raw_comments.csv")
df_processed = processor.full_preprocessing_pipeline(df_spark)

# 3. Analyze sentiment
from sentiment_analyzer import SentimentAnalyzer

analyzer = SentimentAnalyzer()
sentiment_scores = analyzer.analyze_batch(comments_df['text'].tolist())

# 4. Visualize with Streamlit
# Run: streamlit run dashboard.py
```

---

## ğŸ”‘ API Key Setup

### Getting YouTube API Key:
1. Go to [Google Cloud Console](https://console.cloud.google.com/)
2. Create a new project or select existing one
3. Enable **YouTube Data API v3**
4. Create credentials (API Key)
5. Copy the API key to your `.env` file

**Important:** Keep your API key secure and never commit it to version control!

---

## ğŸ“‚ Data Flow

```
YouTube Video
    â†“
[YouTube Data API v3] â† youtube_fetcher.py
    â†“
raw_comments.csv
    â†“
[PySpark Processing] â† data_processor.py
    â†“
[NLTK VADER Sentiment] â† sentiment_analyzer.py
    â†“
processed_sentiment.csv
    â†“
[Streamlit Dashboard] â† dashboard.py
    â†“
Interactive Visualizations
```

---

## ğŸ§  Sentiment Analysis Details

### VADER (Valence Aware Dictionary and sEntiment Reasoner)
- **Compound Score Range:** -1 (most negative) to +1 (most positive)
- **Classification Thresholds:**
  - Positive: compound â‰¥ 0.05
  - Neutral: -0.05 < compound < 0.05
  - Negative: compound â‰¤ -0.05

### Features Extracted:
- **Sentiment Scores:** positive, neutral, negative, compound
- **Engagement Metrics:** likes, replies, engagement_score
- **Temporal Features:** hour_of_day, day_of_week
- **Text Features:** word_count, has_emoji, text_length
- **Author Information:** author_name, comment_id

---

## ğŸš¨ Troubleshooting

### Issue: PySpark Not Found
```bash
pip install pyspark==3.5.7
```

### Issue: Java Not Found
- Install Java 8 or 11
- Set `JAVA_HOME` environment variable

### Issue: NLTK Data Missing
```bash
python -c "import nltk; nltk.download('vader_lexicon'); nltk.download('punkt')"
```

### Issue: YouTube API Quota Exceeded
- YouTube API has daily quota limits (10,000 units/day)
- Each comment fetch costs ~1 unit per comment
- Wait 24 hours or use a different API key

### Issue: Streamlit Port Already in Use
```bash
streamlit run dashboard.py --server.port 8502
```

---

## ğŸ“ˆ Performance Metrics

- **Processing Speed:** ~1000 comments/second with PySpark
- **Memory Usage:** ~500MB for 10,000 comments
- **API Rate Limit:** 10,000 units/day (YouTube API)
- **Dashboard Load Time:** <2 seconds for 50,000 comments

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **PySpark** - Distributed data processing
- **NLTK VADER** - Sentiment analysis lexicon
- **Streamlit** - Interactive dashboard framework
- **YouTube Data API v3** - Comment data source
- **Plotly** - Beautiful visualizations

---

## ğŸ“§ Contact

**Project Maintainer:** Your Name  
**Email:** your.email@example.com  
**GitHub:** [@yourusername](https://github.com/yourusername)

---

## ğŸ”® Future Enhancements

- [ ] Multi-language sentiment analysis
- [ ] Real-time streaming with Kafka
- [ ] Machine learning model for custom sentiment classification
- [ ] Reply thread analysis
- [ ] Sentiment prediction for future comments
- [ ] Integration with other social media platforms
- [ ] Advanced NLP features (topic modeling, entity recognition)
- [ ] Docker containerization
- [ ] Cloud deployment (AWS/GCP/Azure)

---

**â­ If you find this project useful, please consider giving it a star on GitHub!**

